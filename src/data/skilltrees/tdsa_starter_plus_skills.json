{
  "tree": {
    "name": "TDSA Starter Plus",
    "tree": [
      {
        "id": "tdsa_sp_1",
        "title": "Extended metadata repo functionalities",
        "icon": "icons/github.svg",
        "tooltip": {
          "content": "Overview of extended functionalities for metadata repository management."
        },
        "links": [
          {
            "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4667277494/Metadata+Repo+Setup",
            "text": "This Skillstree builds on the last metadata repository with 3 further skill requirements: AWS skills, performing data refreshes (involves a combination of alteryx, python, sql and AWS) and report generation. Have a look at the metadata overview below to remind yourself of the purpose of the repo."
          }
        ],
        "children": [
          {
            "id": "tdsa_sp_2",
            "title": "AWS for metadata repo",
            "icon": "icons/aws.png",
            "tooltip": {
              "content": "Understanding AWS services for metadata repository implementation."
            },
            "links": [
              {
                "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4497014785/1.0+TDSA+Planning",
                "text": "TDSA uses different AWS services for different parts of the web app. Here we will focus on just the services you need to understand for the metadata repository which are: S3 and Aurora. You can see the overview AWS Solution Architecture at the following link."
              }
            ],
            "children": [
              {
                "id": "tdsa_sp_3",
                "title": "AWS documentation",
                "icon": "icons/aws.png",
                "tooltip": {
                  "content": "Dive into AWS documentation to master metadata services."
                },
                "links": [
                  {
                    "url": "https://docs.aws.amazon.com/",
                    "text": "AWS publish guides and a lot of documentation for their services. Here is their official documentation page feel free to browse."
                  }
                ],
                "children": [
                  {
                    "id": "tdsa_sp_4",
                    "title": "AWS cloud computing",
                    "icon": "icons/cloud.svg",
                    "tooltip": {
                      "content": "Learn about AWS cloud computing essentials."
                    },
                    "links": [
                      {
                        "url": "https://jisc365.sharepoint.com/sites/SpecialProjectsTeam/SitePages/AWS.aspx",
                        "text": "There is a brief Sharepoint page to introduce you to the concept of cloud computing and there is the start of team standards at the following link"
                      }
                    ],
                    "children": []
                  },
                  {
                    "id": "tdsa_sp_5",
                    "title": "AWS aurora",
                    "icon": "icons/Aurora.svg",
                    "tooltip": {
                      "content": "Understand how to leverage AWS Aurora for scalable database solutions."
                    },
                    "links": [
                      {
                        "url": "https://aws.amazon.com/rds/aurora/",
                        "text": "We use AWS Aurora to store copies of the data we have outputted from the DataWareHouse. AWS Aurora is a cloud-based relational database service offered by Amazon Web Services (AWS). It is compatible with MySQL and PostgreSQL, which are two very popular open-source relational database systems. In terms of performance, Amazon claims Aurora can deliver up to five times the throughput (amount of data the system can process within a given time frame) of standard MySQL and three times the throughput of standard PostgreSQL. Learn more about Amazon Aurora."
                      }
                    ],
                    "children": []
                  },
                  {
                    "id": "tdsa_sp_6",
                    "title": "AWS S3",
                    "icon": "icons/S3.svg",
                    "tooltip": {
                      "content": "Master storage solutions with AWS S3."
                    },
                    "links": [
                      {
                        "url": "https://aws.amazon.com/s3/",
                        "text": "We use S3 for different purposes for TDSA. One of the uses is to store the outputted tdsa data extract for a customer. Another use related to the metadata repo is to upload the copies of data we have outputted from Data Warehouse - we upload these files as .gzip files to enable smaller compression and faster upload to Aurora. Discover Amazon S3 features at the official page."
                      },
                      {
                        "url": "https://d-936702e084.awsapps.com/start#/",
                        "text": "If you have AWS access go to the S3 service and see if you can locate the following buckets and briefly browse their contents to increase familarity. You will need to logon to AWS then search S3 in the search bar at the top. Then have a look at test-data-fake (here we upload the fake version of the data which is used with the dev version of TDSA), live-data-tdsa (here we upload the real version of the data which is used with both test and prod version of the app), processed-orders-prod (here is where real orders are outputted), processed-orders-test (here is where orders using the live data are outputted but for testing purposes of the codebase/app), processed-orders-dev (here is where fake data orders are outputted) (do not download data)"
                      }
                    ],
                    "children": []
                  }
                ]
              }
            ]
          },
          {
            "id": "tdsa_sp_7",
            "title": "TDSA data refresh",
            "icon": "icons/refresh.svg",
            "tooltip": {
              "content": "Learn the process of refreshing TDSA data for both fake and live data. Currently for live data we save a snapshot of the data from DataWarehouse by running the sql views in DW (code to produce the views is produced in the metadata repo), use Alteryx to save down a copy of each of the views as a csv, then convert to a gzip file. This gzip file is then uploaded into an AWS S3 bucket (manually) and this data is transferred into AWS Aurora using some postgresql after forming some data schemas."
            },
            "links": [
              {
                "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4492328977/Loading+Data+into+AWS",
                "text": "The process is documented in both Confluence and in the GitHub Repos under the READMEs (sql/aurora/fake_data_upload_for_dev/README.md) and (sql/aurora/live_data_upload_for_test_prod/README.md)"
              }
            ],
            "children": [
              {
                "id": "tdsa_sp_8",
                "title": "PGAdmin 4 Setup",
                "icon": "icons/postgresql.png",
                "tooltip": {
                  "content": "Setting up PGAdmin 4 for database management."
                },
                "links": [
                  {
                    "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4510285930/Tools+for+uploading+data+from+local+to+AWS",
                    "text": "In order to run the postgresql needed to extract the data from the gzip in the S3 bucket you need to setup PGAdmin4 with a connection to AWS Aurora. PgAdmin is a management tool for PostgreSQL and derivative relational databases such as AWS RDS and AWS Aurora. It allows you to run queries such as setting up database schema or perform common sql queries of the data. PostgreSQL is an open-source, cross-platform database system known for its advanced features and extensibility, offering powerful performance tuning and custom development options. In contrast, Microsoft SQL Server is a commercial, Windows-focused RDBMS with automated tuning, deep integration features, and its own T-SQL extensions. While migrating from SQL Server to PostgreSQL, expect to navigate differences in query language syntax, indexing, and built-in functions. The steps to setup PgAdmin4 are detailed here. "
                  }
                ],
                "children": [
                  {
                    "id": "tdsa_sp_9",
                    "title": "Fake data refresh",
                    "icon": "icons/fakedata.png",
                    "tooltip": {
                      "content": "Process of refreshing development environments with fake data."
                    },
                    "links": [
                      {
                        "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4492328977/Loading+Data+into+AWS",
                        "text": "The fake data is produced as part of the build_metadata.py scripts where you can input the number of rows you want. The script produces the csv and converts to gzip so you can then drag/drop into the appropriate S3 bucket. The documentation for this process is here and in the repo README (sql/aurora/fake_data_upload_for_dev/README.md)"
                      }
                    ],
                    "children": [
                      {
                        "id": "tdsa_sp_10",
                        "title": "Save gzip to S3",
                        "icon": "icons/gzip.png",
                        "tooltip": {
                          "content": "This is one part of the process to upload fake data. Tick this off once you have performed each part"
                        },
                        "links": [
                          {
                            "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
                            "text": "You can read more about S3 and the different features at the AWS userguide"
                          }
                        ],
                        "children": []
                      },
                      {
                        "id": "tdsa_sp_11",
                        "title": "Create database schema",
                        "icon": "icons/sql.svg",
                        "tooltip": {
                          "content": "Learn how to create an efficient database schema. This is one part of the process to upload fake data."
                        },
                        "links": [
                          {
                            "url": "https://www.postgresql.org/docs/current/ddl-schemas.html",
                            "text": "Database Schema = the structure of a database, including the tables and fields it contains, the relationships between those tables, and the rules to enforce data integrity. It acts as a blueprint for constructing a database and outlines how data is organized and how the different parts of the database relate to each other. By using a script to produce data schemas you can automate the process of creating and updating the database schema as part of our continuous integration and deployment pipeline. This automation can lead to significant time savings and reduce the risk of human error. For comprehensive information on PostgreSQL database schemas and best practices for constructing them, the official PostgreSQL documentation is a good resource. Here’s a link where you can read more about PostgreSQL schemas"
                          }
                        ],
                        "children": []
                      },
                      {
                        "id": "tdsa_sp_12",
                        "title": "Upload to aurora",
                        "icon": "icons/Aurora.svg",
                        "tooltip": {
                          "content": "Able to use Postgresql to upload the fake data from S3 to AWS Aurora."
                        },
                        "links": [
                          {
                            "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Welcome.html",
                            "text": "Uploading data to Amazon Aurora. There is more information on Aurora in this userguide."
                          }
                        ],
                        "children": []
                      }
                    ]
                  },
                  {
                    "id": "tdsa_sp_13",
                    "title": "Live data refresh",
                    "icon": "icons/livedata.png",
                    "tooltip": {
                      "content": "Process of refreshing live environments with actual data."
                    },
                    "links": [
                      {
                        "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4492328977/Loading+Data+into+AWS",
                        "text": "The documentation and process to refresh live data is at the following link. The live data is outputted from Data Warehouse, saved as a csv, converted to gzip and uploaded to S3 within the Jumpbox. Postgresql queries are then used to create database schema for test and prod environments and then to upload the gzip data from S3 to Aurora. Use the following link in combination with the README in the repo (sql/aurora/live_data_upload_for_test_prod/README.md)"
                      }
                    ],
                    "children": [
                      {
                        "id": "tdsa_sp_14",
                        "title": "Save gzip to S3",
                        "icon": "icons/gzip.png",
                        "tooltip": {
                          "content": "This is one part of the process to upload live data. Tick this off once you have performed each part"
                        },
                        "links": [
                          {
                            "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
                            "text": "You can read more about S3 and the different features at the AWS userguide"
                          }
                        ],
                        "children": []
                      },
                      {
                        "id": "tdsa_sp_15",
                        "title": "Create database schema",
                        "icon": "icons/sql.svg",
                        "tooltip": {
                          "content": "Creating database schemas for live data handling."
                        },
                        "links": [
                          {
                            "url": "https://www.postgresql.org/docs/current/ddl-schemas.html",
                            "text": "Database Schema = the structure of a database, including the tables and fields it contains, the relationships between those tables, and the rules to enforce data integrity. It acts as a blueprint for constructing a database and outlines how data is organized and how the different parts of the database relate to each other. By using a script to produce data schemas you can automate the process of creating and updating the database schema as part of our continuous integration and deployment pipeline. This automation can lead to significant time savings and reduce the risk of human error. For comprehensive information on PostgreSQL database schemas and best practices for constructing them, the official PostgreSQL documentation is a good resource. Here’s a link where you can read more about PostgreSQL schemas."
                          }
                        ],
                        "children": []
                      },
                      {
                        "id": "tdsa_sp_16",
                        "title": "Upload to aurora",
                        "icon": "icons/Aurora.svg",
                        "tooltip": {
                          "content": "Able to use Postgresql to upload the live data from S3 to AWS Aurora."
                        },
                        "links": [
                          {
                            "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Welcome.html",
                            "text": "Uploading data to Amazon Aurora. There is more information on Aurora in this userguide."
                          }
                        ],
                        "children": []
                      }
                    ]
                  }
                ]
              }
            ]
          },
          {
            "id": "tdsa_sp_17",
            "title": "Report Generation",
            "icon": "icons/report.svg",
            "tooltip": {
              "content": "Report generation is another feature of the metadata repo, understand its purpose and the code"
            },
            "links": [
              {
                "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4665147433/Reports",
                "text": "Within the metadata repo there is a folder called reports and within this some code to produce an Excel report which shows you the current snapshot of the data and its associated metadata. For instance it outputs each field name, the dataset, definitions, scoring etc. These reports are used to keep track of what fields we have in the data, where we may have gaps in metadata (like definitions) and can be shared with other members of the wider team who are interested in the contents of the data behind tdsa. Read more about TDSA reports at the following confluence page."
              }
            ],
            "children": [
              {
                "id": "tdsa_sp_18",
                "title": "Understand Report Code",
                "icon": "icons/python.svg",
                "tooltip": {
                  "content": "Understand the code used to produce the reports."
                },
                "links": [
                  {
                    "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4665147433/Reports",
                    "text": "Understand the 'releasenotes_reportgeneration_src.py' code and how it uses the jsons as input to produce the reports."
                  }
                ],
                "children": []
              },
              {
                "id": "tdsa_sp_19",
                "title": "Analyse report code",
                "icon": "icons/excel.svg",
                "tooltip": {
                  "content": "Analyse report code"
                },
                "links": [
                  {
                    "url": "https://jiscdev.atlassian.net/wiki/spaces/DD/pages/4665147433/Reports",
                    "text": "Run the report code (highlight code and execute code instead of a run config although you could also set this up) and identify any aspects where metadata may be missing or out of date. Inspect the report to make sure you understand the contents of each column."
                  }
                ],
                "children": []
              }

            ]
          }
        ]
      }
    ]
  }
}
